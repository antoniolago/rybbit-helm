name: Test Helm Chart

on:
  pull_request:
    branches: [main, master]
  repository_dispatch:
    types: [rybbit-release]
  workflow_dispatch:
    inputs:
      rybbit_version:
        description: 'Rybbit version to test (e.g., v1.0.0 or latest)'
        required: false
        default: 'latest'

env:
  KIND_CLUSTER_NAME: rybbit-ci-test
  NAMESPACE: rybbit
  RELEASE_NAME: rybbit

jobs:
  lint:
    name: Lint Helm Chart
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.0

      - name: Run Helm lint
        run: helm lint .

      - name: Run chart-testing lint
        uses: helm/chart-testing-action@v2
        with:
          version: v3.10.0

      - name: Lint chart
        run: ct lint --chart-dirs . --charts . --validate-maintainers=false

  template:
    name: Template Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.0

      - name: Update dependencies
        run: helm dependency update

      - name: Template chart
        run: helm template test-release . --namespace test --debug

      - name: Template with custom values
        run: |
          helm template test-release . \
            --namespace test \
            --set backend.replicaCount=2 \
            --set client.replicaCount=2 \
            --set postgresql.cluster.instances=3 \
            --debug

  integration-test:
    name: Integration Test (KinD)
    runs-on: ubuntu-latest
    needs: [lint, template]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: v3.14.0

      - name: Create KinD cluster
        uses: helm/kind-action@v1
        with:
          cluster_name: ${{ env.KIND_CLUSTER_NAME }}
          config: |
            kind: Cluster
            apiVersion: kind.x-k8s.io/v1alpha4
            nodes:
            - role: control-plane
            - role: worker

      - name: Install CloudNativePG Operator
        run: |
          helm repo add cnpg https://cloudnative-pg.github.io/charts
          helm repo update
          helm install cnpg cnpg/cloudnative-pg \
            --namespace cnpg-system \
            --create-namespace \
            --wait \
            --timeout 300s

      - name: Update Helm dependencies
        run: helm dependency update

      - name: Install Rybbit chart
        run: |
          IMAGE_TAG="${{ github.event.inputs.rybbit_version || github.event.client_payload.version || 'latest' }}"
          
          helm upgrade --install ${{ env.RELEASE_NAME }} . \
            --namespace ${{ env.NAMESPACE }} \
            --create-namespace \
            --set backend.image.tag="${IMAGE_TAG}" \
            --set client.image.tag="${IMAGE_TAG}" \
            --set postgresql.cluster.instances=1 \
            --set clickhouse.clickhouse.replicasCount=1 \
            --set clickhouse.keeper.replicaCount=1 \
            --set backend.replicaCount=1 \
            --set client.replicaCount=1 \
            --set global.podDisruptionBudget.enabled=false \
            --wait \
            --timeout 600s

      - name: Wait for pods
        run: |
          echo "Waiting for all pods to be ready..."
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/instance=${{ env.RELEASE_NAME }} \
            -n ${{ env.NAMESPACE }} \
            --timeout=300s || true
          
          kubectl get pods -n ${{ env.NAMESPACE }}

      - name: Check deployments
        run: |
          kubectl get deployments -n ${{ env.NAMESPACE }}
          
          BACKEND_READY=$(kubectl get deployment ${{ env.RELEASE_NAME }}-backend \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
          
          CLIENT_READY=$(kubectl get deployment ${{ env.RELEASE_NAME }}-client \
            -n ${{ env.NAMESPACE }} \
            -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
          
          echo "Backend ready replicas: $BACKEND_READY"
          echo "Client ready replicas: $CLIENT_READY"

      - name: Show pod logs on failure
        if: failure()
        run: |
          echo "=== Backend logs ==="
          kubectl logs -n ${{ env.NAMESPACE }} -l app.kubernetes.io/component=backend --tail=100 || true
          
          echo "=== Client logs ==="
          kubectl logs -n ${{ env.NAMESPACE }} -l app.kubernetes.io/component=client --tail=100 || true
          
          echo "=== Pod descriptions ==="
          kubectl describe pods -n ${{ env.NAMESPACE }}

      - name: Cleanup
        if: always()
        run: |
          helm uninstall ${{ env.RELEASE_NAME }} -n ${{ env.NAMESPACE }} || true
          kubectl delete namespace ${{ env.NAMESPACE }} --ignore-not-found || true

  k3s-test:
    name: K3s Integration Test
    runs-on: ubuntu-latest
    needs: [lint, template]
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Helm
      uses: azure/setup-helm@v3
      with:
        version: v3.12.3

    - name: Install yq
      run: |
        echo "=== Installing yq ==="
        wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
        chmod +x /usr/local/bin/yq
        echo "✓ yq installed successfully"

    - name: Install k3s
      run: |
        echo "=== Installing k3s ==="
        curl -sfL https://get.k3s.io | sh -
        sudo chmod 644 /etc/rancher/k3s/k3s.yaml
        mkdir -p ~/.kube
        sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
        sudo chown $USER:$USER ~/.kube/config
        export KUBECONFIG=~/.kube/config
        echo "✓ k3s installed successfully"

    - name: Wait for k3s to be ready
      run: |
        echo "=== Waiting for k3s to be ready ==="
        timeout 60s bash -c 'until kubectl get nodes; do sleep 2; done'
        echo "✓ k3s is ready"

    - name: Create test directories
      run: |
        echo "=== Creating test directories ==="
        mkdir -p test/output test/logs
        echo "✓ Test directories created successfully"

    - name: Update Helm dependencies
      run: |
        if [ ! "$(ls -A charts)" ]; then
          echo "=== Updating Helm dependencies ==="
          helm dependency update .
          echo "✓ Helm dependencies updated successfully"
        else
          echo "=== Using existing dependencies ==="
          echo "✓ Using existing dependencies"
        fi

    - name: Generate templates
      id: generate
      run: |
        for test_file in test/test-cases/*.yaml; do
          test_name=$(basename "$test_file" .yaml)
          echo "=== Generating template for: $test_name ==="
          helm template rybbit . -f "$test_file" > "test/output/${test_name}-output.yaml"
          echo "✓ Template generated successfully for $test_name"
        done

    - name: Validate YAML syntax
      run: |
        for test_file in test/test-cases/*.yaml; do
          test_name=$(basename "$test_file" .yaml)
          echo "=== Validating YAML for: $test_name ==="
          if ! yq eval '.' "test/output/${test_name}-output.yaml" > /dev/null 2>&1; then
            echo "Invalid YAML syntax in ${test_name}-output.yaml"
            yq eval '.' "test/output/${test_name}-output.yaml"
            exit 1
          fi
          echo "✓ YAML syntax validated successfully for $test_name"
        done

    - name: Check for unrendered templates
      run: |
        for test_file in test/test-cases/*.yaml; do
          test_name=$(basename "$test_file" .yaml)
          echo "=== Checking for unrendered templates in: $test_name ==="
          if grep -q "{{.*}}" "test/output/${test_name}-output.yaml"; then
            echo "Found unrendered template variables in ${test_name}-output.yaml"
            cat "test/output/${test_name}-output.yaml"
            exit 1
          fi
          echo "✓ No unrendered templates found in $test_name"
        done

    - name: Validate Kubernetes resources
      run: |
        for test_file in test/test-cases/*.yaml; do
          test_name=$(basename "$test_file" .yaml)
          echo "=== Validating Kubernetes resources for: $test_name ==="
          
          if grep -q "kind: ServiceMonitor" "test/output/${test_name}-output.yaml"; then
            if ! kubectl get crd servicemonitors.monitoring.coreos.com >/dev/null 2>&1; then
              echo "Skipping ServiceMonitor validation as Prometheus Operator CRDs are not installed"
              yq eval 'select(.kind != "ServiceMonitor")' "test/output/${test_name}-output.yaml" > "test/output/${test_name}-output-temp.yaml"
              if [ -s "test/output/${test_name}-output-temp.yaml" ]; then
                kubectl apply --dry-run=client -f "test/output/${test_name}-output-temp.yaml"
              else
                echo "No resources to validate after removing ServiceMonitor resources"
              fi
              rm "test/output/${test_name}-output-temp.yaml"
            else
              kubectl apply --dry-run=client -f "test/output/${test_name}-output.yaml"
            fi
          else
            kubectl apply --dry-run=client -f "test/output/${test_name}-output.yaml"
          fi
          echo "✓ Kubernetes resources validated successfully for $test_name"
        done

    - name: Check required resources
      run: |
        for test_file in test/test-cases/*.yaml; do
          test_name=$(basename "$test_file" .yaml)
          echo "=== Checking required resources for: $test_name ==="
          if ! grep -q "kind: Deployment\|kind: Service\|kind: ConfigMap\|kind: Secret" "test/output/${test_name}-output.yaml"; then
            echo "Missing required Kubernetes resources in ${test_name}-output.yaml"
            exit 1
          fi
          echo "✓ Required resources found in $test_name"
        done

    - name: Deploy and test resources
      run: |
        for test_file in test/test-cases/*.yaml; do
          test_name=$(basename "$test_file" .yaml)
          namespace="test-${test_name}-$(date +%m%d%H%M)"
          namespace=$(echo "$namespace" | tr '[:upper:]' '[:lower:]' | tr '_' '-' | cut -c 1-63 | sed 's/-$//')
          
          echo "=== Testing $test_name in namespace: $namespace ==="
          kubectl create namespace "$namespace"
          
          echo "Applying resources..."
          helm template rybbit . -f "$test_file" --namespace "$namespace" | kubectl apply -f - -n "$namespace"
          
          echo "=== Initial Status Check ==="
          echo "Deployments:"
          kubectl get deployments -n "$namespace" -o wide
          
          echo -e "\nPods:"
          kubectl get pods -n "$namespace" -o wide
          
          echo -e "\nRecent Events:"
          kubectl get events -n "$namespace" --sort-by='.lastTimestamp' | tail -n 20
          
          echo -e "\nMonitoring deployments..."
          while true; do
            echo "=== Current Status ==="
            kubectl get deployments,pods -n "$namespace" -o wide
            
            ready_count=$(kubectl get deployments -n "$namespace" -o jsonpath='{.items[*].status.readyReplicas}' | tr ' ' '\n' | grep -v "^$" | wc -l)
            total_count=$(kubectl get deployments -n "$namespace" -o jsonpath='{.items[*].status.replicas}' | tr ' ' '\n' | grep -v "^$" | wc -l)
            
            if [ "$ready_count" -eq "$total_count" ] && [ "$total_count" -gt 0 ]; then
              echo "All deployments are ready!"
              break
            fi
            
            if kubectl get pods -n "$namespace" | grep -q "Error\|CrashLoopBackOff\|ImagePullBackOff\|Terminating"; then
              echo "Found failed or terminating pods. Checking details..."
              kubectl get pods -n "$namespace" | grep -E "Error|CrashLoopBackOff|ImagePullBackOff|Terminating"
              echo -e "\nPod Events:"
              kubectl get events -n "$namespace" --sort-by='.lastTimestamp' | grep -E "Error|Warning|Failed"
              echo -e "\nPod Details:"
              for pod in $(kubectl get pods -n "$namespace" -o jsonpath='{.items[*].metadata.name}'); do
                echo -e "\n=== $pod ==="
                kubectl describe pod -n "$namespace" "$pod" | grep -A 10 "Events:"
              done
              exit 1
            fi
            
            sleep 5
          done
          
          echo "=== Cleaning up ==="
          kubectl delete namespace "$namespace" --force --grace-period=0
          echo "✓ Cleanup completed for $test_name"
        done
